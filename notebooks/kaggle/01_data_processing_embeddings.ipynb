{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ““ Notebook 1: Data Processing & Embeddings (Customer Voice Edition)\n",
                "\n",
                "**Goal:** Ingest multi-source customer data (Reviews + Tweets), clean, chunk, and embed.\n",
                "\n",
                "**Datasets Used:**\n",
                "1.  **Women's E-Commerce Clothing Reviews** (`nicapotato/womens-ecommerce-clothing-reviews`)\n",
                "2.  **Customer Support on Twitter** (`thoughtvector/customer-support-on-twitter`)\n",
                "\n",
                "**Configuration:** Add these datasets to your Kaggle environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q sentence-transformers tiktoken pandas numpy loguru pyarrow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from loguru import logger\n",
                "from tqdm.auto import tqdm\n",
                "import glob\n",
                "\n",
                "# â”€â”€â”€ CONFIGURATION â”€â”€â”€\n",
                "OUTPUT_DIR = \"/kaggle/working\"\n",
                "os.makedirs(f\"{OUTPUT_DIR}/processed\", exist_ok=True)\n",
                "\n",
                "# Sample limits to keep memory in check (adjust based on RAM)\n",
                "REVIEW_LIMIT = 20000  # Full dataset is ~23k\n",
                "TWEET_LIMIT = 50000   # Full dataset is 3M+ (must sample)\n",
                "\n",
                "# â”€â”€â”€ DATA LOADING â”€â”€â”€\n",
                "def load_clothing_reviews():\n",
                "    \"\"\"Load Women's E-Commerce Clothing Reviews\"\"\"\n",
                "    # Look for CSV files in the directory RECURSIVELY\n",
                "    search_path = \"/kaggle/input/**/Womens Clothing E-Commerce Reviews.csv\"\n",
                "    files = glob.glob(search_path, recursive=True)\n",
                "    \n",
                "    # Fallback to broader search if exact name match fails\n",
                "    if not files:\n",
                "         files = glob.glob(\"/kaggle/input/**/womens-ecommerce-clothing-reviews/**/*.csv\", recursive=True)\n",
                "    \n",
                "    if not files:\n",
                "        logger.warning(\"Clothing Reviews dataset not found!\")\n",
                "        return pd.DataFrame()\n",
                "    \n",
                "    logger.info(f"Found clothing reviews file: {files[
                        0
                    ]
                }")\n",
                "    df = pd.read_csv(files[0])\n",
                "    \n",
                "    # Standardize columns\n",
                "    # Dataset cols: ['Unnamed: 0', 'Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name']\n",
                "    df = df.rename(columns={\n",
                "        'Review Text': 'text',\n",
                "        'Rating': 'rating',\n",
                "        'Class Name': 'category',\n",
                "        'Age': 'age',\n",
                "        'Recommended IND': 'recommended'\n",
                "    })\n",
                "    df['source'] = 'clothing_reviews'\n",
                "    \n",
                "    # Create metadata dict with robust null checks\n",
                "    df['metadata'] = df.apply(lambda row: {\n",
                "        'rating': int(row.get('rating', 0)) if pd.notna(row.get('rating')) else 0,\n",
                "        'category': str(row.get('category', 'Unknown')),\n",
                "        'age': int(row.get('age', 0)) if pd.notna(row.get('age')) else 0,\n",
                "        'recommended': int(row.get('recommended', 0)) if pd.notna(row.get('recommended')) else 0,\n",
                "        'source': 'clothing_reviews'\n",
                "    }, axis=1)\n",
                "    \n",
                "    return df[['text', 'metadata']].dropna(subset=['text'])\n",
                "\n",
                "def load_twitter_support():\n",
                "    \"\"\"Load Customer Support Tweets (Sampled)\"\"\"\n",
                "    # Look for CSV files in the directory RECURSIVELY\n",
                "    search_path = \"/kaggle/input/**/twcs.csv\"\n",
                "    files = glob.glob(search_path, recursive=True)\n",
                "\n",
                "    # Fallback\n",
                "    if not files:\n",
                "        files = glob.glob(\"/kaggle/input/**/customer-support-on-twitter/**/*.csv\", recursive=True)\n",
                "\n",
                "    if not files:\n",
                "        logger.warning(\"Twitter Support dataset not found!\")\n",
                "        return pd.DataFrame()\n",
                "    \n",
                "    logger.info(f"Found twitter support file: {files[
                        0
                    ]
                }")\n",
                "    \n",
                "    # Read in chunks or limit rows because this file is HUGE (3M rows)\n",
                "    df = pd.read_csv(files[0], nrows=TWEET_LIMIT)\n",
                "    \n",
                "    # Standardize columns\n",
                "    # Dataset cols: ['tweet_id', 'author_id', 'inbound', 'created_at', 'text', 'response_tweet_id', 'in_response_to_tweet_id']\n",
                "    df = df.rename(columns={'text': 'text', 'author_id': 'author'})\n",
                "    df['source'] = 'twitter_support'\n",
                "    \n",
                "    # Metadata\n",
                "    df['metadata'] = df.apply(lambda row: {\n",
                "        'author': str(row.get('author', 'unknown')),\n",
                "        'in_response_to_tweet_id': str(row.get('in_response_to_tweet_id', '')),\n",
                "        'source': 'twitter_support'\n",
                "    }, axis=1)\n",
                "    \n",
                "    return df[['text', 'metadata']].dropna(subset=['text'])\n",
                "\n",
                "# â”€â”€â”€ MERGE & PROCESS â”€â”€â”€\n",
                "logger.info(\"Loading Datasets...\")\n",
                "df_reviews = load_clothing_reviews()\n",
                "df_tweets = load_twitter_support()\n",
                "\n",
                "logger.info(f\"Reviews: {len(df_reviews)}, Tweets: {len(df_tweets)}\")\n",
                "\n",
                "if df_reviews.empty and df_tweets.empty:\n",
                "    raise ValueError(\"No data found! Please add datasets to input.\")\n",
                "\n",
                "combined_df = pd.concat([df_reviews.head(REVIEW_LIMIT), df_tweets.head(TWEET_LIMIT)], ignore_index=True)\n",
                "logger.info(f\"Total documents to embed: {len(combined_df)}\")\n",
                "\n",
                "# Cleaning (simple)\n",
                "combined_df['text'] = combined_df['text'].astype(str).str.replace('\\n', ' ')\n",
                "\n",
                "# Save chunks\n",
                "meta_df = pd.json_normalize(combined_df['metadata'])\n",
                "export_df = pd.concat([combined_df.drop('metadata', axis=1), meta_df], axis=1)\n",
                "export_df.to_parquet(f\"{OUTPUT_DIR}/all_chunks.parquet\")\n",
                "\n",
                "logger.info(f\"âœ… Saved {len(export_df)} documents to all_chunks.parquet\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€â”€ EMBEDDINGS â”€â”€â”€\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import torch\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "logger.info(f\"Embedding on {device}...\")\n",
                "\n",
                "model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n",
                "\n",
                "embeddings = model.encode(\n",
                "    combined_df['text'].tolist(),\n",
                "    batch_size=256,\n",
                "    show_progress_bar=True,\n",
                "    convert_to_numpy=True\n",
                ")\n",
                "\n",
                "np.save(f\"{OUTPUT_DIR}/embeddings.npy\", embeddings)\n",
                "logger.info(\"Done! Saved chunks and embeddings.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}